{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2fb1063",
   "metadata": {},
   "source": [
    "# Analyze the gamma rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16592181",
   "metadata": {},
   "source": [
    "## Defining the Surrogate model for the gamma rule\n",
    "\n",
    "### Linear layer\n",
    "\n",
    "We look at the simple case of a linear model, that shares its parameters in every layer. \n",
    "We have a matrix $A^{(t)} \\in \\mathbb{R}^{n \\times n}$ and define\n",
    "\n",
    "$$ h^{(0)} = x_0 $$\n",
    "$$ h^{(t)} = A^{(t)} h^{(t-1)} $$\n",
    "\n",
    "for some initialization $x_0 \\in \\mathbb{R}^{n}$.\n",
    "Instead of the intuitive surrogate model with $R^+_{\\cdot | \\cdot}(t) = A^{(t)}$,\n",
    "we would now like to define an alternative surrogate model, that constructs the same layerwise activations $ R^+_{\\cdot}(t) $ but with different transition coefficients $R^+_{j | i}(t)$.\n",
    "\n",
    "**Forward surrogate model.** \n",
    "We choose a $\\gamma \\in \\mathbb{R}^+$, and abbreviate $w_{j, i} = A^{(t)}_{j, i}$ for the current layer $t$.\n",
    "$w_{j | i}^+$ is the positive part of the weights $w_{j | i}$.\n",
    "\n",
    "$$ R^+(0) = x_0 $$\n",
    "$$ R^+_{j | i}(t) = (w_{j | i} + \\gamma \\cdot w_{j | i}^+) \\cdot c_j $$\n",
    "$$ c_j = \\frac{ \\sum_i R^+_{i}(t-1) w_{j | i} }{ \\sum_i R^+_{i}(t-1) (w_{j | i} + \\gamma \\cdot w_{j | i}^+) } $$\n",
    "\n",
    "where $c_j \\in [0, 1]$ is a scaling factor per output neuron $j$.\n",
    "\n",
    "**Surrogate model check** We want to check, how good the surrogate model recovers the original mode, we want to show that $R^+(t) = h(t)$.\n",
    "\n",
    "*Proof.* We show by induction, starting with $t = 0$. By definition,\n",
    "\n",
    "$$ R^+(0) = x_0 = h^{(0)} $$\n",
    "\n",
    "Now, in the induction step $t \\rightarrow t+1$, we assume $ R^+(t-1) = h^{(t-1)}$, then\n",
    "\n",
    "\\begin{align*}\n",
    "R^+_{j}(t) \n",
    "&= \\sum_i R^+_{j | i}(t) \\cdot R^+_{i}(t-1) \\\\\n",
    "&= \\sum_i R^+_{i}(t-1) \\cdot (w_{j | i} + \\gamma \\cdot w_{j | i}^+) \\cdot c_j \\\\\n",
    "&= c_j  \\cdot \\sum_i R^+_{i}(t-1) \\cdot (w_{j | i} + \\gamma \\cdot w_{j | i}^+) \\\\\n",
    "&= \\frac{ \\sum_i R^+_{i}(t-1) w_{j | i} }{ \\sum_i R^+_{i}(t-1) (w_{j | i} + \\gamma \\cdot w_{j | i}^+) }\n",
    "         \\cdot \\sum_i R^+_{i}(t-1) \\cdot (w_{j | i} + \\gamma \\cdot w_{j | i}^+) \\\\\n",
    "&= \\sum_i R^+_{i}(t-1) w_{j | i} \\\\\n",
    "&= \\sum_i h^{(t-1)} w_{j | i} \\\\\n",
    "&= h^{(t)} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "**Backward surrogate model.**\n",
    "For the backward chain we have \n",
    "\n",
    "\\begin{align*}\n",
    "R^-_{i}(t-1) \n",
    "&= \\sum_j \\frac{ R^+_{j | i}(t) \\cdot R^+_{i}(t-1) }\n",
    "                { \\sum_{i'} R^+_{j | i'}(t) \\cdot R^+_{i'}(t-1)} \n",
    "            R^-_{j}(t) \\\\\n",
    "&= \\sum_j \\frac{ (w_{j | i} + \\gamma \\cdot w_{j | i}^+) \\cdot c_j \\cdot h_{i}^{(t-1)} }\n",
    "                { \\sum_{i'} (w_{j | i'} + \\gamma \\cdot w_{j | i'}^+) \\cdot c_j \\cdot h_{i'}^{(t-1)} } \n",
    "            R^-_{j}(t) \\\\\n",
    "&= \\sum_j \\frac{ (w_{j | i} + \\gamma \\cdot w_{j | i}^+) \\cdot h_{i}^{(t-1)} }\n",
    "                { \\sum_{i'} (w_{j | i'} + \\gamma \\cdot w_{j | i'}^+) \\cdot h_{i'}^{(t-1)} } \n",
    "            R^-_{j}(t) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "which is known as the LRP-$\\gamma$ rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e64b88b",
   "metadata": {},
   "source": [
    "**LRP-0 rule**\n",
    "\n",
    "\\begin{align*}\n",
    "R_{j}\n",
    "&= \\sum_k \\frac{ a_{j} \\cdot w_{jk}  }\n",
    "                { \\sum_{j'} a_{j} \\cdot w_{j'k} } \n",
    "            R_{k} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "**Gamma rule**\n",
    "\n",
    "\\begin{align*}\n",
    "R_{j}\n",
    "&= \\sum_k \\frac{ a_{j} \\cdot (w_{jk} + \\gamma \\cdot w_{jk}^+) }\n",
    "                { \\sum_{j'} a_{j'} \\cdot (w_{j'k} + \\gamma \\cdot w_{j'k}^+) } \n",
    "            R_{k} \\\\\n",
    "&= \\sum_k \\frac{ a_{j} \\cdot (w_{jk} + \\gamma \\cdot w_{jk}^+) }\n",
    "                { \\sum_{j'} a_{j'} \\cdot w_{j'k} + \\gamma \\cdot \\sum_{j'} a_{j'} \\cdot w_{j'k}^+ } \n",
    "            R_{k} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "**Divisor of Gamma rule**\n",
    "\\begin{align*}\n",
    "\\sum_{j'} a_{j} \\cdot (w_{j'k} + \\gamma \\cdot w_{j'k}^+) \n",
    "&= \\sum_{j'} a_{j'} \\cdot w_{j'k} + \\gamma \\cdot \\sum_{j'} a_{j'} \\cdot w_{j'k}^+ \n",
    "\\end{align*}\n",
    "\n",
    "**Make Gamma dependent on incoming activations and weights of neuron**\n",
    "\n",
    "**and output-neuron specific**\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{j'} a_{j'} \\cdot w_{j'k} + \\gamma((a_{j'})_{j'}, (w_{j'k})_{j'}) \\cdot \\sum_{j'} a_{j'} \\cdot w_{j'k}^+ \n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603c6c41",
   "metadata": {},
   "source": [
    "**LRP-$\\gamma$ rule as a linear transformation**\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "R_{j}\n",
    "&= \\sum_k \\underbrace{\\frac\n",
    "                { a_{j} \\cdot (w_{jk} + \\gamma \\cdot w_{jk}^+) }\n",
    "                { \\sum_{j'} a_{j'} \\cdot (w_{j'k} + \\gamma \\cdot w_{j'k}^+) }\n",
    "            }_{:= R_{j|k}}\n",
    "            R_{k} \\\\\n",
    "&= \\sum_k  R_{j|k} \\cdot R_{k} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "**Matrix form**\n",
    "\\begin{align*}\n",
    "\\vec{(R_{j})}_{j} \n",
    "&= (R_{j|k})_{jk} \\cdot \\vec{(R_{k})}_{k} \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3282bb5c",
   "metadata": {},
   "source": [
    "**Gamma rule**\n",
    "\n",
    "\\begin{align*}\n",
    "R_{j}\n",
    "&= \\sum_k \\frac{ a_{j} \\cdot (w_{jk} + \\gamma \\cdot w_{jk}^+) }\n",
    "                { \\sum_{j'} a_{j'} \\cdot (w_{j'k} + \\gamma \\cdot w_{j'k}^+) } \n",
    "            R_{k} \\\\\n",
    "\\end{align*}\n",
    "\n",
    ".\n",
    "\n",
    "**Divisor of Gamma rule**\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{j'} a_{j} \\cdot (w_{j'k} + \\gamma \\cdot w_{j'k}^+) \n",
    "&= \\sum_{j'} a_{j'} \\cdot w_{j'k} + \\gamma \\cdot \\sum_{j'} a_{j'} \\cdot w_{j'k}^+ \n",
    "&= z_k + \\gamma \\cdot z_k^+\n",
    "\\end{align*}\n",
    "\n",
    "We were interesed in those $\\gamma$ where the Divisor becomes 0 and changes and, following, changes its sign.\n",
    "We thought these $\\gamma$ should not be overstepped, to ensure\n",
    "Thus followed the Gamma-per-Neuron rule:\n",
    "\n",
    "\\begin{align*}\n",
    "\\gamma_k^{max}  =  - \\frac{z_k}{z_k^+}; \\;\\;\\;\\;\\;\\;\\;\\; 0 \\leq \\gamma_k \\leq max(0, \\gamma_k^{max})\n",
    "\\end{align*}\n",
    "\n",
    "The issue is that the constructed rule only has any effect for neurons where $z_k < 0 \\Rightarrow a_k = 0 \\Rightarrow R_k = 0$.\n",
    "The conditional relevancy may be modified, but will always be multiplied by a prior relevancy $R_k = 0$.\n",
    "\n",
    "In detail, there are three possible cases:\n",
    "1. $z_k < 0$. In this case the following ReLU makes the output neurons activation $a_k = 0$ and thus relevancy $R_k = 0$.\n",
    "2. $z_k = z_k^+$. This means all incoming weights are positive.\n",
    "3. $0 < z_k < z_k^+$. This means some weights are negative. \n",
    "\n",
    "#3 is the only case that interests us, and where the gamma rule will have any effect.\n",
    "\n",
    "## Summary\n",
    "\n",
    "- The current LRP-\"Gamma per Neuron\" is equivalent to LRP-0, because all neurons that would be transformed in a relevant way by LRP-gamma get \"clamped away\". (We don't want to apply a negative gamma.)\n",
    "    - There might be a way to gain insight of the ratio $\\frac{z_k}{z_k^+}$, but this is not as nicely motivated as was \"avoiding signflips\".\n",
    "\n",
    "- LRP-gamma is equaivalent to \"LRP-gamma wo signflips\" because those output neurons where signflips would occur have $a_k = R_k = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9aedd3",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- The current LRP-\"Gamma per Neuron\" is equivalent to LRP-0, because all neurons that would be transformed in a relevant way by LRP-gamma get \"clamped away\". (We don't want to apply a negative gamma.)\n",
    "    - There might be a way to gain insight of the ratio $\\frac{z_k}{z_k^+}$, but this is not as nicely motivated as was \"avoiding signflips\".\n",
    "\n",
    "- LRP-gamma is equaivalent to \"LRP-gamma wo signflips\" because those output neurons where signflips would occur have $a_k = R_k = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9967bb",
   "metadata": {},
   "source": [
    "## Next steps?\n",
    "\n",
    "- Eigenvalue analysis is much cleaner if we filter out the (irrelevant) output-neurons that have sign flips.\n",
    "- -> Precise analysis how EVs change with gamma/epsilon\n",
    "- Explicit EV manipulation in Back matrix\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1bb996",
   "metadata": {},
   "source": [
    "## Organizational, \"Arbeitsstruktur\"\n",
    "\n",
    "\n",
    "\n",
    "### Als Forschungsthema\n",
    "- geil, wird Thomas auf jeden Fall weiter dran arbeiten\n",
    "\n",
    "### Msc\n",
    "- Goal: Regel um gamma optimal zu finden\n",
    "- Gute Note: Präzise arbeiten, durchdacht, gut ausformuliert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cbfd82",
   "metadata": {},
   "source": [
    "## Next steps.\n",
    "\n",
    "- EVs for joint rel matrix.\n",
    "- EVs for conditional rel matrix, where irrelevant, \"exploding\" rows are frozen.\n",
    "\n",
    "## Thomas\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b084e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94515e9b",
   "metadata": {},
   "source": [
    "### Lipschitz continuity\n",
    "\n",
    "The difference between the relevancies $R_j^{(1)}$, $R_j^{(2)}$ in layer $(l)$,\n",
    "for two different explanations $(1)$, $(2)$ can be bounded wrt.\n",
    "\n",
    "the difference between the relevancies $R_k^{(1)}$, $R_k^{(2)}$ in layer $(l+1)$:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "\\| R_j^{(1)} - R_j^{(2)} \\| &= \\| \\text{\\footnotesize{LRP}}_{k \\rightarrow j}(R_k^{(1)})  - \\text{\\footnotesize{LRP}}_{k \\rightarrow j}(R_k^{(2)}) \\| \\\\\n",
    "                          &= \\| R_{j|k} \\cdot R_k^{(1)}           - R_{j|k} \\cdot R_k^{(2)} \\| \\\\\n",
    "                          &= \\| R_{j|k} \\cdot ( R_k^{(1)} - R_k^{(2)} ) \\| \\\\\n",
    "                          &\\leq \\underbrace{\\| R_{j|k} \\|}_\\text{\\large{L}} \\cdot \\| R_k^{(1)} - R_k^{(2)} \\| \\\\\n",
    "\n",
    "\n",
    "\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "(Abbreviated indices; $R_k^{(1)}$, $R_k^{(2)}$ are in vector form, $R_{j\\|k}$ is in matrix form.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b273e197",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "0494f27bb39db6b3d215c7cce3d65be35d8071cb380b71d0665c33206dc0f891"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
